{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from osgeo import gdal, ogr\n",
    "from osgeo.gdalconst import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "gdal.PushErrorHandler('CPLQuietErrorHandler')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critical loads\n",
    "\n",
    "The critical loads workflow needs updating this year. This notebook documents some intiial exploration.\n",
    "\n",
    "## 1. Vegetation map\n",
    "\n",
    "One operation that has previously taken a lot of time is estimating exceedances based on vegetation cover. As a test, I'd like to calculate summary land use statistics for each cell in the BLR grid. If this works reasonably quickly, we should be able to do everything else we need fairly easily.\n",
    "\n",
    "To begin with, I'll run tests uisng the 60 m resolution data, as this is easier to manipulate initially.\n",
    "\n",
    "### 1.1. Mosiac to a single dataset\n",
    "\n",
    "The raw, 60 m vegetation data is here:\n",
    "\n",
    "K:\\Avdeling\\317 Klima- og milj√∏modellering\\KAU\\Focal Centre\\Vegetation\\Veg map\\satveg_30\\1\n",
    "\n",
    "I've copied this locally and used the `Mosiac_To_New_Raster` tool in ArcToolbox to combine the tiles into a single 8-bit integer GeoTiff (`sat_veg_60m_all.tif`). This file has an uncompressed size of 557 MB (which implies the 30 m dataset will be about 2.2 GB). We should be able to work with these grids fairly efficiently, but note that if I need to reclassify the land uses to decimal values for the critical loads calculations, the file sizes will increase by a factor of 4, as they will need to be upcast to 32-bit floats.\n",
    "\n",
    "For now, I will focus on calculating some simple zonal statistics, as this will give an indication of performance.\n",
    "\n",
    "### 1.2. Read land use lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read lookup table\n",
    "in_xlsx = (r'C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads'\n",
    "           r'\\sat_veg_land_use_classes.xlsx')\n",
    "df = pd.read_excel(in_xlsx, sheetname='EUNIS_tilGIS', index_col=0)\n",
    "\n",
    "# Get cols of interest\n",
    "df = df[['EUNISveg',]]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Reproject BLR grid\n",
    "\n",
    "The BLR grid is stored in WGS84 geographic co-ordinates, whereas the vegetation data is all in UTM Zone 33N projected co-ordinates (based on the WGS84 datum). I have therefore reprojected the BLR grid to match the ratser data, as this will improve performance by eliminating the need for \"on-the-fly\" reprojection. The reprojected dataset is here:\n",
    "\n",
    "C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads\\GIS\\Shapefiles\\blrgrid_uten_grums_utm_z33n.shp\n",
    "\n",
    "### 1.4. Zonal statistics code\n",
    "\n",
    "The code below is modified from [here](https://gist.github.com/perrygeo/5667173) and provides low-level access to GDAL, which should be substantially faster than ArcGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remap_categories(category_map, stats):\n",
    "    \"\"\" Modified from https://gist.github.com/perrygeo/5667173\n",
    "        Original code copyright 2013 Matthew Perry\n",
    "    \"\"\"\n",
    "    def lookup(m, k):\n",
    "        \"\"\" Dict lookup but returns original key if not found\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return m[k]\n",
    "        except KeyError:\n",
    "            return k\n",
    "\n",
    "    return {lookup(category_map, k): v\n",
    "            for k, v in stats.items()}\n",
    "\n",
    "def bbox_to_pixel_offsets(gt, bbox):\n",
    "    \"\"\" Modified from https://gist.github.com/perrygeo/5667173\n",
    "        Original code copyright 2013 Matthew Perry\n",
    "    \"\"\"\n",
    "    originX = gt[0]\n",
    "    originY = gt[3]\n",
    "    pixel_width = gt[1]\n",
    "    pixel_height = gt[5]\n",
    "    x1 = int((bbox[0] - originX) / pixel_width)\n",
    "    x2 = int((bbox[1] - originX) / pixel_width) + 1\n",
    "\n",
    "    y1 = int((bbox[3] - originY) / pixel_height)\n",
    "    y2 = int((bbox[2] - originY) / pixel_height) + 1\n",
    "\n",
    "    xsize = x2 - x1\n",
    "    ysize = y2 - y1\n",
    "    \n",
    "    return (x1, y1, xsize, ysize)\n",
    "\n",
    "def zonal_stats(vector_path, raster_path, nodata_value=None, global_src_extent=False,\n",
    "                categorical=False, category_map=None):\n",
    "    \"\"\" Modified from https://gist.github.com/perrygeo/5667173\n",
    "        Original code copyright 2013 Matthew Perry\n",
    "    \"\"\"\n",
    "    rds = gdal.Open(raster_path, GA_ReadOnly)\n",
    "    assert(rds)\n",
    "    rb = rds.GetRasterBand(1)\n",
    "    rgt = rds.GetGeoTransform()\n",
    "\n",
    "    if nodata_value:\n",
    "        nodata_value = float(nodata_value)\n",
    "        rb.SetNoDataValue(nodata_value)\n",
    "\n",
    "    vds = ogr.Open(vector_path, GA_ReadOnly)  # TODO maybe open update if we want to write stats\n",
    "    assert(vds)\n",
    "    vlyr = vds.GetLayer(0)\n",
    "\n",
    "    # create an in-memory numpy array of the source raster data\n",
    "    # covering the whole extent of the vector layer\n",
    "    if global_src_extent:\n",
    "        # use global source extent\n",
    "        # useful only when disk IO or raster scanning inefficiencies are your limiting factor\n",
    "        # advantage: reads raster data in one pass\n",
    "        # disadvantage: large vector extents may have big memory requirements\n",
    "        src_offset = bbox_to_pixel_offsets(rgt, vlyr.GetExtent())\n",
    "        src_array = rb.ReadAsArray(*src_offset)\n",
    "\n",
    "        # calculate new geotransform of the layer subset\n",
    "        new_gt = (\n",
    "            (rgt[0] + (src_offset[0] * rgt[1])),\n",
    "            rgt[1],\n",
    "            0.0,\n",
    "            (rgt[3] + (src_offset[1] * rgt[5])),\n",
    "            0.0,\n",
    "            rgt[5]\n",
    "        )\n",
    "\n",
    "    mem_drv = ogr.GetDriverByName('Memory')\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "\n",
    "    # Loop through vectors\n",
    "    stats = []\n",
    "    feat = vlyr.GetNextFeature()\n",
    "    while feat is not None:\n",
    "\n",
    "        if not global_src_extent:\n",
    "            # use local source extent\n",
    "            # fastest option when you have fast disks and well indexed raster (ie tiled Geotiff)\n",
    "            # advantage: each feature uses the smallest raster chunk\n",
    "            # disadvantage: lots of reads on the source raster\n",
    "            src_offset = bbox_to_pixel_offsets(rgt, feat.geometry().GetEnvelope())\n",
    "            src_array = rb.ReadAsArray(*src_offset)\n",
    "\n",
    "            # calculate new geotransform of the feature subset\n",
    "            new_gt = (\n",
    "                (rgt[0] + (src_offset[0] * rgt[1])),\n",
    "                rgt[1],\n",
    "                0.0,\n",
    "                (rgt[3] + (src_offset[1] * rgt[5])),\n",
    "                0.0,\n",
    "                rgt[5]\n",
    "            )\n",
    "\n",
    "        # Create a temporary vector layer in memory\n",
    "        mem_ds = mem_drv.CreateDataSource('out')\n",
    "        mem_layer = mem_ds.CreateLayer('poly', None, ogr.wkbPolygon)\n",
    "        mem_layer.CreateFeature(feat.Clone())\n",
    "\n",
    "        # Rasterize it\n",
    "        rvds = driver.Create('', src_offset[2], src_offset[3], 1, gdal.GDT_Byte)\n",
    "        rvds.SetGeoTransform(new_gt)\n",
    "        gdal.RasterizeLayer(rvds, [1], mem_layer, burn_values=[1])\n",
    "        rv_array = rvds.ReadAsArray()\n",
    "\n",
    "        # Mask the source data array with our current feature\n",
    "        # we take the logical_not to flip 0<->1 to get the correct mask effect\n",
    "        # we also mask out nodata values explictly\n",
    "        masked = np.ma.MaskedArray(\n",
    "            src_array,\n",
    "            mask=np.logical_or(\n",
    "                src_array == nodata_value,\n",
    "                np.logical_not(rv_array)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if categorical:\n",
    "            # Get cell counts for each category\n",
    "            keys, counts = np.unique(masked.compressed(), return_counts=True)\n",
    "            pixel_count = dict(zip([np.asscalar(k) for k in keys],\n",
    "                                   [np.asscalar(c) for c in counts]))\n",
    "\n",
    "            feature_stats = dict(pixel_count)\n",
    "            if category_map:\n",
    "                feature_stats = remap_categories(category_map, feature_stats)\n",
    "        \n",
    "        else:\n",
    "            # Get summary stats\n",
    "            feature_stats = {\n",
    "                'min': float(masked.min()),\n",
    "                'mean': float(masked.mean()),\n",
    "                'max': float(masked.max()),\n",
    "                'std': float(masked.std()),\n",
    "                'sum': float(masked.sum()),\n",
    "                'count': int(masked.count()),\n",
    "                'fid': int(feat.GetFID())}                        \n",
    "                        \n",
    "        stats.append(feature_stats)\n",
    "\n",
    "        rvds = None\n",
    "        mem_ds = None\n",
    "        feat = vlyr.GetNextFeature()\n",
    "\n",
    "    vds = None\n",
    "    rds = None\n",
    "    \n",
    "    return pd.DataFrame(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Run zonal statistics\n",
    "\n",
    "First, convert the Excel land cover classes to a dictionary that maps the land use codes to class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get mapping of LU codes to land classes\n",
    "category_map = df.to_dict()['EUNISveg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the zonal stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Input datasets\n",
    "in_shp = (r'C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads'\n",
    "          r'\\GIS\\Shapefiles\\blrgrid_uten_grums_utm_z33n.shp')\n",
    "\n",
    "in_tif = (r'C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads'\n",
    "          r'\\GIS\\Raster\\sat_veg_60m_all.tif')\n",
    "\n",
    "# Zonal stats\n",
    "zs = zonal_stats(in_shp, in_tif, categorical=True, \n",
    "                 category_map=category_map)\n",
    "\n",
    "# Convert cell counts to areas in km2\n",
    "zs = zs*60.*60./1.E6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zonal stats processing takes just under 6 seconds, which implies we should also be able to process the 30 m dataset without too much difficulty (although 30 m seems unnecessary if we're interested in the whole of Norway).\n",
    "\n",
    "### 1.6. Combine with BLR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read BLR grid\n",
    "blr = gpd.read_file(in_shp)\n",
    "\n",
    "# Convert blr area to km2\n",
    "blr['blr_area_km2'] = blr['area_m2'] / 1.E6\n",
    "\n",
    "# Delete unwanted cols\n",
    "del blr['geometry'], blr['area_m2']\n",
    "\n",
    "# Convert to df\n",
    "blr = pd.DataFrame(blr)\n",
    "\n",
    "# Rename cols\n",
    "blr.columns = ['blr_id', 'blr_area_km2']\n",
    "\n",
    "# Join\n",
    "res = blr.join(zs)\n",
    "\n",
    "# Melt to long format\n",
    "res = pd.melt(res, id_vars=['blr_id', 'blr_area_km2'], \n",
    "              var_name='land_use',\n",
    "              value_name='area_km2')\n",
    "\n",
    "# Set multi-index\n",
    "res.sort_values(['blr_id', 'land_use'], inplace=True)\n",
    "res.set_index(['blr_id', 'land_use'], inplace=True)\n",
    "\n",
    "res.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write to CSV\n",
    "out_csv = (r'C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads\\blr_land_use_props.csv')\n",
    "res.to_csv(out_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CL_meq/m2/yr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NORUTcode</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CL_meq/m2/yr\n",
       "NORUTcode              \n",
       "1                    36\n",
       "2                    36\n",
       "3                    36\n",
       "4                    71\n",
       "5                    71"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read lookup table\n",
    "in_xlsx = (r'C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads'\n",
    "           r'\\sat_veg_land_use_classes.xlsx')\n",
    "df = pd.read_excel(in_xlsx, sheetname='EUNIS_tilGIS', index_col=0)\n",
    "\n",
    "df = df[['CL_meq/m2/yr']].round(0).astype(int)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reclassify_raster(in_tif, out_tif, reclass_df, reclass_col, ndv):\n",
    "    \"\"\" Reclassify categorical values in a raster using a mapping\n",
    "        in a dataframe. The dataframe index must contain the classes\n",
    "        in in_tif and the 'reclass_col' must specify the new classes.\n",
    "    \"\"\"\n",
    "    from osgeo import gdal, ogr\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # Open source file, read data\n",
    "    src_ds = gdal.Open(in_tif, GA_ReadOnly)\n",
    "    assert(src_ds)\n",
    "    rb = src_ds.GetRasterBand(1)\n",
    "    rgt = src_ds.GetGeoTransform()\n",
    "    data = rb.ReadAsArray()\n",
    "    \n",
    "    # Reclassify\n",
    "    for idx, row in reclass_df.iterrows():\n",
    "        data[data==idx] = row[reclass_col]\n",
    "\n",
    "    # Write output\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    dst_ds = driver.CreateCopy(out_tif, src_ds, 0)\n",
    "    out_band = dst_ds.GetRasterBand(1)\n",
    "    out_band.SetNoDataValue(ndv)\n",
    "    out_band.WriteArray(data)\n",
    "\n",
    "    # Flush data and close datasets\n",
    "    dst_ds = None\n",
    "    src_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_tif = (r'C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads'\n",
    "           r'\\GIS\\Raster\\sat_veg_30m_all.tif')\n",
    "\n",
    "out_tif = (r'C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads'\n",
    "           r'\\GIS\\Raster\\sat_veg_30m_cr_lds.tif')\n",
    "\n",
    "reclassify_raster(in_tif, out_tif, df, 'CL_meq/m2/yr', 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert BLR grid to high resolution raster\n",
    "\n",
    "https://gis.stackexchange.com/questions/91772/converting-shapefile-to-raster-character-attributes\n",
    "\n",
    "http://osgeo-org.1560.x6.nabble.com/gdal-dev-shifting-or-snapping-one-raster-to-another-td4647150.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from osgeo import gdal, ogr\n",
    "\n",
    "in_shp = (r'C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads'\n",
    "          r'\\GIS\\Shapefiles\\blrgrid_uten_grums_utm_z33n.shp')\n",
    "\n",
    "in_tif = (r'C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads'\n",
    "          r'\\GIS\\Raster\\sat_veg_30m_all.tif')\n",
    "\n",
    "out_tif = (r'C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads'\n",
    "           r'\\GIS\\Raster\\blr_30m.tif')\n",
    "\n",
    "\n",
    "# Open source file, read data\n",
    "src_ras = gdal.Open(in_tif, GA_ReadOnly)\n",
    "rgt = src_ras.GetGeoTransform()\n",
    "\n",
    "# Dataset properties\n",
    "originX = rgt[0]   # Origin is top-left corner\n",
    "originY = rgt[3]   # i.e. (xmin, ymax)\n",
    "pixelWidth = rgt[1]\n",
    "pixelHeight = -rgt[5]\n",
    "cols = src_ras.RasterXSize\n",
    "rows = src_ras.RasterYSize\n",
    "\n",
    "# Calculate extent\n",
    "xmin = int(originX)\n",
    "xmax = int(originX + cols*pixelWidth)\n",
    "ymin = int(originY + rows*pixelHeight)\n",
    "ymax = int(originY)\n",
    "    \n",
    "dst_ds = gdal.GetDriverByName('GTiff').Create(out_tif, cols, rows, 1, gdal.GDT_UInt32) \n",
    "dst_ds.SetGeoTransform((xmin, pixelWidth, 0, ymin, 0, pixelHeight))\n",
    "\n",
    "band = dst_ds.GetRasterBand(1)\n",
    "band.SetNoDataValue(0)\n",
    "band.FlushCache()\n",
    "\n",
    "#driver = gdal.GetDriverByName('GTiff')\n",
    "#dst_ds = driver.CreateCopy(out_tif, src_ras, 0)\n",
    "#\n",
    "#rb = dst_ds.GetRasterBand(1)\n",
    "#data = rb.ReadAsArray()\n",
    "#data = np-zeros(shape=data.shape)\n",
    "#rb.WriteArray(data)\n",
    "\n",
    "src_vec = ogr.Open(in_shp)\n",
    "src_lyr = src_vec.GetLayer()\n",
    "\n",
    "gdal.RasterizeLayer(dst_ds, [1], src_lyr, options=['ATTRIBUTE=BLR']) \n",
    "#band = dst_ds.GetRasterBand(1)\n",
    "#band.GetStatistics(0,1)\n",
    "\n",
    "# Flush data and close datasets\n",
    "dst_ds = None\n",
    "src_vec = None\n",
    "src_lyr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from osgeo import ogr\n",
    "from osgeo import gdal\n",
    "\n",
    "aoi_uri = (r'C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads'\n",
    "           r'\\GIS\\Raster\\sat_veg_30m_all.tif')\n",
    "aoi_raster = gdal.Open(aoi_uri)\n",
    "\n",
    "def new_raster_from_base(base, outputURI, format, nodata, datatype):\n",
    "\n",
    "    cols = base.RasterXSize\n",
    "    rows = base.RasterYSize\n",
    "    projection = base.GetProjection()\n",
    "    geotransform = base.GetGeoTransform()\n",
    "\n",
    "    driver = gdal.GetDriverByName(format)\n",
    "\n",
    "    new_raster = driver.Create(outputURI, cols, rows, 1, datatype)\n",
    "    new_raster.SetProjection(projection)\n",
    "    new_raster.SetGeoTransform(geotransform)\n",
    "\n",
    "    new_raster.GetRasterBand(1).SetNoDataValue(nodata)\n",
    "    new_raster.GetRasterBand(1).Fill(nodata)\n",
    "\n",
    "    return new_raster\n",
    "\n",
    "shape_uri = (r'C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads'\n",
    "             r'\\GIS\\Shapefiles\\blrgrid_uten_grums_utm_z33n.shp')\n",
    "shape_datasource = ogr.Open(shape_uri)\n",
    "shape_layer = shape_datasource.GetLayer()\n",
    "\n",
    "raster_out = (r'C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads'\n",
    "              r'\\GIS\\Raster\\blr_30m.tif')\n",
    "\n",
    "raster_dataset = new_raster_from_base(aoi_raster, raster_out, 'GTiff',\n",
    "                                      0, gdal.GDT_UInt32)\n",
    "band = raster_dataset.GetRasterBand(1)\n",
    "nodata = band.GetNoDataValue()\n",
    "band.Fill(nodata)\n",
    "\n",
    "gdal.RasterizeLayer(raster_dataset, [1], shape_layer, options=['ATTRIBUTE=BLR'])\n",
    "\n",
    "raster_dataset = None\n",
    "shape_layer = None\n",
    "aoi_raster = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
