{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import nivapy3 as nivapy\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import critical_loads as cl\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critcal Loads: Dockerised workflow (November 2018)\n",
    "\n",
    "This notebook begins the process of transferring the Critical Loads workflow out of RESA and into Docker, so that it can eventually be moved onto the new data platform. A workflow is provided for creating a new PostGIS database in Docker and populating it with key datasets required for the critical loads calculations. Some data exploration and checking is also presented.\n",
    "\n",
    "## 1. Create a PostGIS database running on Docker\n",
    "\n",
    "The following code has been used to create a PostGIS database running on my (local) Docker host.\n",
    "\n",
    "    # Create data volume\n",
    "    docker volume create pg_data_crit_lds\n",
    "    \n",
    "    # Create container\n",
    "    docker run --name=postgis_crit_lds -d -e POSTGRES_USER=admin -e POSTGRES_PASS=pw1234 -e POSTGRES_DBNAME=critical_loads -e ALLOW_IP_RANGE=0.0.0.0/0 -p 25432:5432 -v pg_data_crit_lds:/var/lib/postgresql kartoza/postgis:9.6-2.4\n",
    "    \n",
    "**Note:** Once the container has been created, it can be stopped/started at any time using\n",
    "\n",
    "    # Start\n",
    "    docker start postgis_crit_lds\n",
    "    \n",
    "    # Stop\n",
    "    docker stop postgis_crit_lds\n",
    "    \n",
    "We can now start the DS Toolkit and establish connections to the following databases:\n",
    "\n",
    " 1. **RESA2**. A centralised NIVA database containing most of the old (non-spatial) data relating to the Critical Loads project\n",
    " \n",
    " 2. **PostGIS running on `localhost`**. My local PostGIS installation, which stores some of the new spatial datasets required for the updated calculations\n",
    " \n",
    " 3. **PostGIS running on Docker**. The new database created for the updated workflow. We wish to transfer all the Critical Loads datasets (both spatial and non-spatial) to this database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to the NIVABASE\n",
    "ora_eng = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to local PostGIS\n",
    "loc_pg_eng = nivapy.da.connect(src='postgres',\n",
    "                               db_name='niva_work',\n",
    "                               port=5432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to Docker PostGIS\n",
    "doc_pg_eng = nivapy.da.connect(src='postgres',\n",
    "                               db_name='critical_loads',\n",
    "                               port = 25432)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deposition data\n",
    "\n",
    "First, create a new schema to store deposition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f6dac7b5cc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create deposition schema\n",
    "sql = \"CREATE SCHEMA deposition\"\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Transfer BLR and 0.1 degree deposition grids\n",
    "\n",
    "Deposition data from NILU is now supplied on a 0.1 degree grid, which is currently stored in my local PostGIS installation. The code below transfers it to the new schema in Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f6df00fb390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 0.1 degree dep grid from PostGIS\n",
    "sql = (\"SELECT * FROM public.dep_grid_0_1deg\")\n",
    "dep_gdf = gpd.read_postgis(sql, loc_pg_eng)\n",
    "\n",
    "# Write to new db\n",
    "nivapy.da.gdf_to_postgis(dep_gdf, \n",
    "                         'dep_grid_0_1deg', \n",
    "                         'deposition', \n",
    "                         doc_pg_eng,\n",
    "                         'dep_dep_grid_0_1deg_spidx',\n",
    "                         if_exists='replace',\n",
    "                         index=False)\n",
    "\n",
    "# Use 'cell_id' col as primary key\n",
    "sql = (\"ALTER TABLE deposition.dep_grid_0_1deg \"\n",
    "       \"ADD CONSTRAINT dep_grid_0_1deg_pk \"\n",
    "       \"PRIMARY KEY (cell_id)\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous years, data have been supplied using the BLR grid, which is available as a shapefile. The code below loads this into the database as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! The dataframe contains mixed geometries:\n",
      "  {'MultiPolygon', 'Polygon'}\n",
      "These will be cast to \"Multi\" type. If this is not what you want, consider using gdf.explode() first\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f6da1ad1e10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read BLR .shp\n",
    "blr_path = (r'../../../../GIS/shapefiles/blrgrid_uten_grums.shp')\n",
    "dep_gdf = gpd.read_file(blr_path)\n",
    "dep_gdf.rename({'BLR':'blr'},\n",
    "               axis=1,\n",
    "               inplace=True)\n",
    "\n",
    "# Write to new db\n",
    "nivapy.da.gdf_to_postgis(dep_gdf, \n",
    "                         'dep_grid_blr', \n",
    "                         'deposition', \n",
    "                         doc_pg_eng,\n",
    "                         'dep_dep_grid_blr_spidx',\n",
    "                         if_exists='replace',\n",
    "                         index=False)\n",
    "\n",
    "# Use 'blr' col as primary key\n",
    "sql = (\"ALTER TABLE deposition.dep_grid_blr \"\n",
    "       \"ADD CONSTRAINT dep_grid_blr_pk \"\n",
    "       \"PRIMARY KEY (blr)\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Transfer \"Deposition Series Definitions\" from the NIVABASE\n",
    "\n",
    "In RESA2, the table `DEP_SERIES_DEFINITIONS` identifies the datasets previously supplied by NILU. Each dataset has a separate row (and ID) in this table. For consistency, I will transfer this table directly and use the same dataset IDs in the new workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24</td>\n",
       "      <td>expost gridav MFR2020-Update</td>\n",
       "      <td>Lagt inn i forb. med ex-Post analyser 2010 For...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>Middel 2007-2011</td>\n",
       "      <td>Fordelt til BLR av NILU 2012 (Wenche Aas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>Middel 2012-2016</td>\n",
       "      <td>Fordelt til BLR av NILU 2017 (Wenche Aas; old ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27</td>\n",
       "      <td>Middel 2012-2016 (new)</td>\n",
       "      <td>Fordelt til BLR av NILU 2017 (Wenche Aas; new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28</td>\n",
       "      <td>Middel 2012-2016 (new; hi-res)</td>\n",
       "      <td>Fordelt til BLR av NILU 2017 (Wenche Aas; new ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    series_id                            name  \\\n",
       "14         24    expost gridav MFR2020-Update   \n",
       "15         25                Middel 2007-2011   \n",
       "27         26                Middel 2012-2016   \n",
       "16         27          Middel 2012-2016 (new)   \n",
       "17         28  Middel 2012-2016 (new; hi-res)   \n",
       "\n",
       "                                          description  \n",
       "14  Lagt inn i forb. med ex-Post analyser 2010 For...  \n",
       "15          Fordelt til BLR av NILU 2012 (Wenche Aas)  \n",
       "27  Fordelt til BLR av NILU 2017 (Wenche Aas; old ...  \n",
       "16  Fordelt til BLR av NILU 2017 (Wenche Aas; new ...  \n",
       "17  Fordelt til BLR av NILU 2017 (Wenche Aas; new ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from RESA\n",
    "sql = \"SELECT * FROM resa2.dep_series_definitions\"\n",
    "df = pd.read_sql(sql, ora_eng)\n",
    "\n",
    "# Tidy\n",
    "df.rename({'dep_series_id':'series_id'},\n",
    "          axis=1,\n",
    "          inplace=True)\n",
    "df.sort_values('series_id', inplace=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f6da1ad1048>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write to new db\n",
    "df.to_sql('dep_series_defs', \n",
    "          doc_pg_eng,\n",
    "          'deposition',\n",
    "          if_exists='replace',\n",
    "          index=False)\n",
    "\n",
    "# Use 'dep_series_id' col as primary key\n",
    "sql = (\"ALTER TABLE deposition.dep_series_defs \"\n",
    "       \"ADD CONSTRAINT dep_series_defs_pk \"\n",
    "       \"PRIMARY KEY (series_id)\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Transfer \"Deposition Parameter Definitions\" from the NIVABASE\n",
    "\n",
    "In RESA2, the table `AIR_PARAMETER_DEFINITIONS` identifies various deposition parameters. As far as I can see, only the first three or four entries in this table are currently relevant. Many of the columns also seem unnecessary at present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_id</th>\n",
       "      <th>name</th>\n",
       "      <th>unit</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>N(oks)</td>\n",
       "      <td>mg N/m2/year</td>\n",
       "      <td>Oxidized nitrogen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>N(red)</td>\n",
       "      <td>mg N/m2/year</td>\n",
       "      <td>Reduced nitogen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>mg S/m2/year</td>\n",
       "      <td>Total sulphur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>S*</td>\n",
       "      <td>mg S/m2/year</td>\n",
       "      <td>Non marine sulphur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_id    name          unit          description\n",
       "0         1  N(oks)  mg N/m2/year    Oxidized nitrogen\n",
       "1         2  N(red)  mg N/m2/year      Reduced nitogen\n",
       "2         3       S  mg S/m2/year        Total sulphur\n",
       "3         4      S*  mg S/m2/year  Non marine sulphur "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from RESA\n",
    "sql = (\"SELECT * FROM resa2.air_parameter_definitions \"\n",
    "       \"WHERE parameter_id < 5\")\n",
    "df = pd.read_sql(sql, ora_eng)\n",
    "\n",
    "# Tidy\n",
    "df.sort_values('parameter_id', inplace=True)\n",
    "df.drop(['formula', 'category', 'function', 'entered_by', 'entered_date'],\n",
    "        axis=1,\n",
    "        inplace=True)\n",
    "df.rename({'parameter_id':'param_id'}, \n",
    "          inplace=True,\n",
    "          axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f6da1ad4a20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write to new db\n",
    "df.to_sql('dep_param_defs', \n",
    "          doc_pg_eng,\n",
    "          'deposition',\n",
    "          if_exists='replace',\n",
    "          index=False)\n",
    "\n",
    "# Use 'dep_series_id' col as primary key\n",
    "sql = (\"ALTER TABLE deposition.dep_param_defs \"\n",
    "       \"ADD CONSTRAINT dep_param_defs_pk \"\n",
    "       \"PRIMARY KEY (param_id)\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Data for \"Deposition Values\"\n",
    "\n",
    "#### 2.4.1. Create tables\n",
    "\n",
    "The old workflow used the coarse BLR grid for deposition values. Deposition `series_ids` from 1 to 27 are all based on this grid, so these values will be transferred to a new table named `'dep_values_blr_grid'`. Deposition `series_id=28` uses the new 0.1 degree grid, so this dataset (and subsequent ones) will need adding to a separate table (`'dep_values_0_1deg_grid'`). As future data will also be delivered using this grid, it will be useful to have a function for processing and adding raw data to this table.\n",
    "\n",
    "Additionally, both these tables need constraints to ensure that relevant series and parameter IDs are defined before uploading data. Note that I have not included a constraint on the BLR/cell ID, because the data supplied by NILU often includes cells that are outside of terrestrial Norway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f6da16bf588>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete if already exist\n",
    "sql = (\"DROP TABLE IF EXISTS deposition.dep_values_blr_grid, \"\n",
    "       \"  deposition.dep_values_0_1deg_grid\")\n",
    "doc_pg_eng.execute(sql)\n",
    "\n",
    "# Create table for BLR data\n",
    "sql = (\"CREATE TABLE deposition.dep_values_blr_grid \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  blr integer NOT NULL, \"\n",
    "       \"  param_id integer NOT NULL, \"\n",
    "       \"  value numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, blr, param_id), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION, \"\n",
    "       \"  CONSTRAINT param_id_fkey FOREIGN KEY (param_id) \"\n",
    "       \"      REFERENCES deposition.dep_param_defs (param_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "doc_pg_eng.execute(sql)\n",
    "\n",
    "# Create table for 0.1 degree data\n",
    "sql = (\"CREATE TABLE deposition.dep_values_0_1deg_grid \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  cell_id integer NOT NULL, \"\n",
    "       \"  param_id integer NOT NULL, \"\n",
    "       \"  value numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, cell_id, param_id), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION, \"\n",
    "       \"  CONSTRAINT param_id_fkey FOREIGN KEY (param_id) \"\n",
    "       \"      REFERENCES deposition.dep_param_defs (param_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2. Transfer \"old\" data\n",
    "\n",
    "The old data using the BLR grid are stored in RESA2 in the table `DEP_BLR_VALUES`. We'll just transfer the data for `param_ids` 1 to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from RESA\n",
    "sql = (\"SELECT * FROM resa2.dep_blr_values \"\n",
    "       \"WHERE parameter_id < 5\")\n",
    "df = pd.read_sql(sql, ora_eng)\n",
    "\n",
    "# Tidy\n",
    "df.rename({'dep_series_id':'series_id',\n",
    "           'parameter_id':'param_id'},\n",
    "          axis=1,\n",
    "          inplace=True)\n",
    "\n",
    "# Write to new db\n",
    "df.to_sql('dep_values_blr_grid', \n",
    "          doc_pg_eng,\n",
    "          'deposition',\n",
    "          if_exists='append',\n",
    "          index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3. Import \"new\" data\n",
    "\n",
    "In 2017, NILU supplied raw data for the 0.1 degree grid in `.dat` format. I previously wrote some code ([here](http://nbviewer.jupyter.org/github/JamesSample/critical_loads/blob/master/notebooks/critical_loads_workflow_new_grid.ipynb#1.1.-Upload-new-data-to-database)) to process this data. I have now generalised this and moved it into a new `.py` file, which will eventually become a Python package/module for the Critical Loads workflow.\n",
    "\n",
    "**Note:** Double-check with Kari that the NILU file `tot_s_combined-emep_grid.dat` contains data for *non-marine* S. We assumed this in the 2018 analysis, but worth checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>param_id</th>\n",
       "      <th>value</th>\n",
       "      <th>series_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50050305</td>\n",
       "      <td>2</td>\n",
       "      <td>270.87</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50050315</td>\n",
       "      <td>2</td>\n",
       "      <td>240.50</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50050325</td>\n",
       "      <td>2</td>\n",
       "      <td>259.19</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50050335</td>\n",
       "      <td>2</td>\n",
       "      <td>252.06</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50050345</td>\n",
       "      <td>2</td>\n",
       "      <td>332.82</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cell_id param_id   value  series_id\n",
       "0  50050305        2  270.87         28\n",
       "1  50050315        2  240.50         28\n",
       "2  50050325        2  259.19         28\n",
       "3  50050335        2  252.06         28\n",
       "4  50050345        2  332.82         28"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process NILU data and add to db\n",
    "nilu_fold = r'../../../../raw_data/2012_2016'\n",
    "df = cl.upload_nilu_0_1deg_dep_data(nilu_fold, \n",
    "                                    doc_pg_eng,\n",
    "                                    28)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exceedance for vegatation\n",
    "\n",
    "First, create a new schema for data relating to the vegetation calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f6da1566358>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create veg schema\n",
    "sql = \"CREATE SCHEMA vegetation\"\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Transfer critical loads table for vegetation\n",
    "\n",
    "The Excel file \n",
    "\n",
    "    Critical_Loads\\vegetation\\sat_veg_land_use_classes.xlsx\n",
    "    \n",
    "contains critical load values for various vegetation classes. The original is in the sheet named `'EUNIS_tilGIS'`, but I have also created a tidied version (with e.g. lower case column names) in the sheet named `'eunis_lower_case'`.\n",
    "\n",
    "As above, if this table is likely to change, it should be normalised and loaded into the database more carefully. For now, the \"flat\" Excel format is convenient, but I nevertheless want to store it in the database with the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f6da19c2630>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Excel data\n",
    "xl_path = r'../../../../vegetation/sat_veg_land_use_classes.xlsx'\n",
    "df = pd.read_excel(xl_path, sheet_name='eunis_lower_case')\n",
    "\n",
    "# Write to new db\n",
    "df.to_sql('land_class_crit_lds', \n",
    "          doc_pg_eng,\n",
    "          'vegetation',\n",
    "          if_exists='replace',\n",
    "          index=False)\n",
    "\n",
    "# Use 'norut_code' as primary key\n",
    "sql = (\"ALTER TABLE vegetation.land_class_crit_lds \"\n",
    "       \"ADD CONSTRAINT veg_land_class_crit_lds_pk \"\n",
    "       \"PRIMARY KEY (norut_code)\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Transfer \"critical loads for vegetation\" grid\n",
    "\n",
    "The code in Section 1.3 of [this notebook](http://nbviewer.jupyter.org/github/JamesSample/critical_loads/blob/master/notebooks/critical_loads_workflow_new_grid.ipynb#1.3.-Reclassify) converted 30 m resolution satellite vegetation data into a grid of critical loads (based on the Excel table above - Section 3.1). Unless we update the vegetation data, this calculation does not need to be performed again, but it would be useful to store the resulting raster grid (`sat_veg_30m_cr_lds_div100.tif`) in the PostGIS database.  \n",
    "\n",
    "It is not yet possible to load raster data via the Toolkit, because I haven't installed PostGIS and all the associated extensions in the container. However, I do have PostGIS installed on the Docker host, so I can load data into Docker PostGIS from there. The procedure is as follows:\n",
    "\n",
    " 1. Open a Windows `CMD` prompt and `cd` into\n",
    " \n",
    "         C:\\Program Files\\PostgreSQL\\9.6\\bin \n",
    "         \n",
    "    <br> (or add `raster2pgsql` to your `PATH`).\n",
    "    \n",
    " 2. Run the following code\n",
    " \n",
    "         raster2pgsql -s 32633 -d -C -I -M -l 2,4,8,16,32,64,128,256 full\\path\\to\\raster.tif -t 100x100 vegetation.sat_veg_30m_all | psql -h localhost -U admin -p 25432 -d critical_loads\n",
    "    \n",
    "    <br>You will be prompted to enter the password for the Docker database. This command specifies the following options (more details [here](https://postgis.net/docs/using_raster_dataman.html)):\n",
    "    \n",
    "     * **-s**. EPSG code for spatial reference\n",
    "     * **-d**. Drop ratser if already exists in database\n",
    "     * **-C**. Apply raster constraints -- srid, pixelsize etc. to ensure raster is properly registered\n",
    "     * **-I**. Build a spatial index\n",
    "     * **-M**. Vaccum analyze after loading\n",
    "     * **-l**. Build pyramids (8 levels)\n",
    "     * **-t**. Cut raster into tiles to be inserted one per table row\n",
    " \n",
    "    <br>The part after `psql` then specifies the connection details for the Docker database.     \n",
    "   \n",
    "**Note:** PostGIS can store very large rasters, but exploring them with QGIS via DB Manager can cause problems - either the database connection gets repeatedly interrupted, or QGIS crashes. NivaPy includes functions for reading PostGIS raster data into either (i) Numpy arrays, using `nivapy.da.postgis_raster_to_array()`, or (ii) GeoTiffs, using `nivapy.da.postgis_raster_to_geotiff()` (the latter function returns the data as a Numpy array as well). For large datasets, it may be better to export the data as a GeoTiff and then add it to QGIS. Alternatively, perform any spatial operations *within the database* first to generate a small set of results, then visualise these via DB Manager in QGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XXX. Exceedance for water\n",
    "\n",
    "First, create a new schema for data relating to the water calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f6da15665c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create water schema\n",
    "sql = \"CREATE SCHEMA water\"\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XXX.1. MAGIC model output for regression\n",
    "\n",
    "Kari has a spreadsheet here\n",
    "\n",
    "    K:\\Avdeling\\317 Klima- og miljømodellering\\KAU\\Focal Centre\\Data\\bc0regresjonNOK_TL2005-rapport_KAU.xls\n",
    "    \n",
    "containing output from the MAGIC model. This is used to generate parameters for the calculation of critical loads to water. It is unclear whether this data will be updated in the future. If so, the dataset should be properly normalised before adding to the database. However, for now I'm assuming that this is a static dataset, so I'm just athe essential data to the database as a \"flat\" table for the purposes of data storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f6da1920fd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read tidied Excel data\n",
    "xl_path = r'../../../../water/update_nov_2018/regression_data_tidied.xls'\n",
    "df = pd.read_excel(xl_path, sheet_name='tidied')\n",
    "\n",
    "# Write to new db\n",
    "df.to_sql('magic_regression_data', \n",
    "          doc_pg_eng,\n",
    "          'water',\n",
    "          if_exists='replace',\n",
    "          index=False)\n",
    "\n",
    "# Use ('resa_stn_id', 'sim_yr') as primary key\n",
    "sql = (\"ALTER TABLE water.magic_regression_data \"\n",
    "       \"ADD CONSTRAINT water_magic_regression_data_pk \"\n",
    "       \"PRIMARY KEY (resa_stn_id, sim_yr)\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XXX.2. Perform regression\n",
    "\n",
    "In the MAGIC data table, values for `bc_x_k` in 1860 are used to define $BC^*_0$, and values from 1986 are used to define $BC^*$. The regression uses $BC^*$ as the x-variable and $BC^*_0$ as the y-variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    BC0   R-squared:                       0.985\n",
      "Model:                            OLS   Adj. R-squared:                  0.985\n",
      "Method:                 Least Squares   F-statistic:                     5320.\n",
      "Date:                Wed, 21 Nov 2018   Prob (F-statistic):           1.20e-75\n",
      "Time:                        14:05:57   Log-Likelihood:                -211.76\n",
      "No. Observations:                  83   AIC:                             427.5\n",
      "Df Residuals:                      81   BIC:                             432.4\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.2744      0.536      0.512      0.610      -0.792       1.340\n",
      "BC             0.9431      0.013     72.938      0.000       0.917       0.969\n",
      "==============================================================================\n",
      "Omnibus:                       26.839   Durbin-Watson:                   1.981\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              202.031\n",
      "Skew:                          -0.530   Prob(JB):                     1.35e-44\n",
      "Kurtosis:                      10.569   Cond. No.                         64.4\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "sql = (\"SELECT resa_stn_id, sim_yr, bc_x_k \"\n",
    "       \"FROM water.magic_regression_data\")\n",
    "df = pd.read_sql(sql, doc_pg_eng)\n",
    "df.index = df['resa_stn_id']\n",
    "del df['resa_stn_id']\n",
    "\n",
    "# Split by year\n",
    "bc0_df = df.query('sim_yr == 1860')\n",
    "del bc0_df['sim_yr']\n",
    "bc0_df.columns = ['BC0']\n",
    "\n",
    "bc_df = df.query('sim_yr == 1986')\n",
    "del bc_df['sim_yr']\n",
    "bc_df.columns = ['BC']\n",
    "\n",
    "# Join\n",
    "df = bc0_df.join(bc_df)\n",
    "\n",
    "# Regression\n",
    "res = sm.ols(formula='BC0 ~ BC', data=df).fit()\n",
    "\n",
    "print (res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same result as in Kari's Excel spreadsheet. Note, however, that the intercept is not significantly different from zero. This implies an alternative, \"slope-only\" model *might* be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    BC0   R-squared:                       0.994\n",
      "Model:                            OLS   Adj. R-squared:                  0.994\n",
      "Method:                 Least Squares   F-statistic:                 1.310e+04\n",
      "Date:                Wed, 21 Nov 2018   Prob (F-statistic):           3.12e-92\n",
      "Time:                        14:06:00   Log-Likelihood:                -211.89\n",
      "No. Observations:                  83   AIC:                             425.8\n",
      "Df Residuals:                      82   BIC:                             428.2\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "BC             0.9481      0.008    114.449      0.000       0.932       0.965\n",
      "==============================================================================\n",
      "Omnibus:                       28.577   Durbin-Watson:                   1.977\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              189.981\n",
      "Skew:                          -0.682   Prob(JB):                     5.58e-42\n",
      "Kurtosis:                      10.285   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "res = sm.ols(formula='BC0 ~ BC - 1', data=df).fit()\n",
    "print (res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is better (lower AIC and BIC) than the original, and it's also simpler. I therefore propose using\n",
    "\n",
    "$$BC^*_0 = 0.9481BC^*_t$$\n",
    "\n",
    "instead of \n",
    "\n",
    "$$BC^*_0 = 0.9431BC^*_t + 0.2744$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
